# Atlas API Configuration
ATLAS_API_URL=http://localhost:8000
ATLAS_USERNAME=simulation@atlas.local
ATLAS_PASSWORD=your-password-here
ATLAS_TIMEOUT=30.0
ATLAS_MAX_RETRIES=3

# =============================================================================
# LLM PROVIDER SELECTION
# =============================================================================
# Choose your LLM provider: claude, openai, gemini, ollama, lm_studio
# Using "ollama" or "lm_studio" runs models locally (free, no API costs!)
LLM_PROVIDER=claude

# =============================================================================
# CLOUD API KEYS (required if using cloud providers)
# =============================================================================
# Get keys from:
#   - Anthropic: https://console.anthropic.com/
#   - OpenAI: https://platform.openai.com/api-keys
#   - Google: https://aistudio.google.com/apikey (FREE tier available!)
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=...

# Cloud Model Selections (defaults to cheapest models - Jan 2026)
# Cost estimates per 1M tokens:
#   - claude-haiku-4-5: $1.00 input / $5.00 output (fastest Claude)
#   - gpt-5-nano: $0.05 input / $0.40 output (cheapest OpenAI)
#   - gemini-2.5-flash: FREE (15 RPM, 500 RPD) or $0.15 input / $0.60 output
CLAUDE_MODEL=claude-haiku-4-5
GPT_MODEL=gpt-5-nano
GEMINI_MODEL=gemini-2.5-flash

# =============================================================================
# LOCAL LLM - OLLAMA (http://localhost:11434)
# =============================================================================
# Run local models with Ollama: https://ollama.ai
# Install: curl -fsSL https://ollama.ai/install.sh | sh
# Start: ollama serve
# Pull model: ollama pull qwen3:30b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:30b

# Available models (from `ollama ls`):
#   - qwen3:30b (18GB) - Best for general chat + tool calling
#   - qwen3-coder:30b (18GB) - Good for coding tasks
#   - devstral-small-2 (15GB) - Fast, good for simple agents
#   - GLM-4.5-Air:Q4_K_M (72GB) - High quality but slow

# =============================================================================
# LOCAL LLM - LM STUDIO (http://localhost:1234/v1)
# =============================================================================
# LM Studio provides an OpenAI-compatible API for local models
# Download: https://lmstudio.ai
# Start the local server in LM Studio, then set the model name below
LM_STUDIO_BASE_URL=http://localhost:1234/v1
LM_STUDIO_MODEL=

# =============================================================================
# LLM Parameters (optional)
# =============================================================================
LLM_MAX_TOKENS=4096
LLM_TEMPERATURE=0.7

# =============================================================================
# WebSocket Server (optional)
# =============================================================================
WS_HOST=0.0.0.0
WS_PORT=8765

# =============================================================================
# Simulation (optional)
# =============================================================================
SIMULATION_SPEED=1.0
DAY_DURATION_SECONDS=300.0
LOG_LEVEL=INFO

# =============================================================================
# Economics (optional)
# =============================================================================
INFLATION_ANNUAL_RATE=0.025
INFLATION_START_DATE=2024-01-01
